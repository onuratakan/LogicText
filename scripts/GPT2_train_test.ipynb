{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_train_test.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Pzxl1vYX-1kk",
        "iLXW02eIYpcB",
        "0p--9zwqQRTc",
        "vS1RJJDFOPnb"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzxl1vYX-1kk",
        "colab_type": "text"
      },
      "source": [
        "##Important Notes:\n",
        "\n",
        "1) Make sure GPU is enabled, go to edit->notebook settings->Hardware Accelerator GPU\n",
        "\n",
        "2) Make a copy to your google drive, click on copy to drive in panel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW0abT07ZkhZ",
        "colab_type": "text"
      },
      "source": [
        "Note: Colab will reset after 12 hours make sure to save your model checkpoints to google drive around 10-11 hours mark or before, then go to runtime->reset all runtimes. Now copy your train model back into colab and start training again from the previous checkpoint. [More info](https://stackoverflow.com/questions/55050988/can-i-run-a-google-colab-free-edition-script-and-then-shutdown-my-computer)\n",
        "\n",
        "Thanks [@ak9250](https://github.com/ak9250/gpt-2-colab/blob/master/GPT_2.ipynb) and [@nshepperd](https://github.com/nshepperd/gpt-2) for sharing their codes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcYmFJC1FP3N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "3098fbde-c708-4201-facc-b327351eef52"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Nov 20 11:27:05 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLXW02eIYpcB",
        "colab_type": "text"
      },
      "source": [
        "## 1. Setting Up\n",
        "\n",
        "Clone and cd into repo, nshepperd's fork https://github.com/nshepperd/gpt-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICYu3w9hIJkC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "a92aa106-cfc7-4545-e541-f2cc3777ff64"
      },
      "source": [
        "!git clone https://github.com/nshepperd/gpt-2.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 310, done.\u001b[K\n",
            "remote: Total 310 (delta 0), reused 0 (delta 0), pack-reused 310\u001b[K\n",
            "Receiving objects: 100% (310/310), 4.40 MiB | 3.04 MiB/s, done.\n",
            "Resolving deltas: 100% (167/167), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eEIs3ApZUVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e500e739-590b-44de-a7f5-72c8211bc60b"
      },
      "source": [
        "cd gpt-2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtn1qZPgZLb0",
        "colab_type": "text"
      },
      "source": [
        "Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "434oOx0bZH6J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "0457f602-017e-464d-bc01-7521e35e23db"
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.7MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
            "\u001b[?25hCollecting toposort==1.5\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=6ad555f2510dcfc82ec78c179193ceb46433fd8fab0ae4ea032854b1000b4078\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533175 sha256=cfa128bab2011fe0a79c1a70f0ef29170328bbc4b185500a0123f9539aac3e3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "Installing collected packages: fire, regex, tqdm, toposort\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.2.1 regex-2017.4.5 toposort-1.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvUQhgK3PQ4L",
        "colab_type": "text"
      },
      "source": [
        "Mount drive to access google drive for saving and accessing checkpoints later. Have to log in to your google account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNpf6R4ahYSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1hrgeKFYsuE",
        "colab_type": "text"
      },
      "source": [
        "Download the model data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A498TySgHYyF",
        "colab": {}
      },
      "source": [
        "!python3 download_model.py 117M"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UDpEGjfO8Q2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "2e12056e-d912-4fb4-b782-2b8b26482a25"
      },
      "source": [
        "!python3 download_model.py 345M"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 822kit/s]                                                      \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 62.0Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 845kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:17, 79.4Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 7.39Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 61.0Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 53.6Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq-YwRnNOBYO",
        "colab_type": "text"
      },
      "source": [
        "encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oJPQtdLbbeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KzSbAvePgsI",
        "colab_type": "text"
      },
      "source": [
        "Fetch checkpoints if you have them saved in google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA2Wk7yIPmS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/drive/My\\ Drive/checkpoint/ /content/gpt-2/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p--9zwqQRTc",
        "colab_type": "text"
      },
      "source": [
        "## 2. Training\n",
        "Let's get our train on! In this case the file is ```dataset.txt``` in your drive folder. To do this, just rename your text file as ```dataset.txt``` and copy to root of your google drive.\n",
        "\n",
        "\n",
        "For small datasets(< 10 MB) you will have to be sure not to run the fine-tuning for too long or you will overfit badly. Roughly, expect interesting results within minutes to hours in the 1-10s of megabyte ballpark, and below this you may want to stop the run early as fine-tuning can be very fast.\n",
        "\n",
        "Big datasets (>100 MB) might not fit the all GPUs of colab. If you get a memory error, either restart runtime until you get gpu with 16 GB memory(P100 or T40), or make your dataset smaller."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxzyTDn9IDjz",
        "colab_type": "text"
      },
      "source": [
        "Copy ```dataset.txt``` from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KijJQu80Hv3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/drive/My\\ Drive/dataset.txt dataset.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPfJ5b3CQXqr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Start training, --model_name '345M' uses the 345 model. Default learning rate is 0.00002, it is good for starting fine tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEn_ihcGI00T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "072d182e-0298-4af0-bde4-8531320ac7e8"
      },
      "source": [
        "!PYTHONPATH=src ./train.py --dataset dataset.txt --model_name '345M' --learning_rate 0.00002"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-11-20 09:26:32.317281: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-11-20 09:26:32.319539: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2d75480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2019-11-20 09:26:32.319587: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2019-11-20 09:26:32.325258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2019-11-20 09:26:32.539441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-20 09:26:32.540162: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2d74bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2019-11-20 09:26:32.540207: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-11-20 09:26:32.541449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-20 09:26:32.542013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-11-20 09:26:32.549171: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-11-20 09:26:32.777997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2019-11-20 09:26:32.916565: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2019-11-20 09:26:32.941640: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2019-11-20 09:26:33.178057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2019-11-20 09:26:33.202136: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2019-11-20 09:26:33.641823: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-11-20 09:26:33.642022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-20 09:26:33.642716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-20 09:26:33.643253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2019-11-20 09:26:33.647540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2019-11-20 09:26:33.649005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-11-20 09:26:33.649036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2019-11-20 09:26:33.649047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2019-11-20 09:26:33.651257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-20 09:26:33.651922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-11-20 09:26:33.652487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:166: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From ./train.py:117: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:89: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From ./train.py:156: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "Loading dataset...\n",
            "100% 1/1 [02:38<00:00, 158.95s/it]\n",
            "dataset has 30820922 tokens\n",
            "Training...\n",
            "2019-11-20 09:29:59.573228: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "[1 | 11.42] loss=3.85 avg=3.85\n",
            "[2 | 12.95] loss=2.95 avg=3.40\n",
            "[3 | 14.51] loss=2.85 avg=3.22\n",
            "[4 | 16.07] loss=3.00 avg=3.16\n",
            "[5 | 17.63] loss=1.94 avg=2.91\n",
            "[6 | 19.19] loss=3.06 avg=2.94\n",
            "[7 | 20.77] loss=2.76 avg=2.91\n",
            "[8 | 22.34] loss=2.48 avg=2.86\n",
            "[9 | 23.92] loss=4.55 avg=3.05\n",
            "[10 | 25.51] loss=3.13 avg=3.06\n",
            "[11 | 27.10] loss=3.33 avg=3.09\n",
            "[12 | 28.71] loss=3.08 avg=3.09\n",
            "[13 | 30.31] loss=3.37 avg=3.11\n",
            "[14 | 31.91] loss=2.86 avg=3.09\n",
            "[15 | 33.52] loss=2.36 avg=3.04\n",
            "[16 | 35.13] loss=3.22 avg=3.05\n",
            "[17 | 36.75] loss=3.04 avg=3.05\n",
            "[18 | 38.38] loss=2.70 avg=3.03\n",
            "[19 | 40.02] loss=2.37 avg=2.99\n",
            "[20 | 41.64] loss=2.62 avg=2.97\n",
            "[21 | 43.27] loss=3.04 avg=2.97\n",
            "[22 | 44.90] loss=2.71 avg=2.96\n",
            "[23 | 46.54] loss=3.54 avg=2.99\n",
            "[24 | 48.19] loss=3.14 avg=2.99\n",
            "[25 | 49.84] loss=2.28 avg=2.96\n",
            "[26 | 51.50] loss=2.83 avg=2.96\n",
            "[27 | 53.17] loss=4.56 avg=3.02\n",
            "[28 | 54.85] loss=2.65 avg=3.01\n",
            "[29 | 56.54] loss=2.73 avg=3.00\n",
            "[30 | 58.22] loss=2.53 avg=2.98\n",
            "[31 | 59.92] loss=2.50 avg=2.96\n",
            "[32 | 61.63] loss=2.51 avg=2.95\n",
            "[33 | 63.33] loss=2.11 avg=2.92\n",
            "[34 | 65.03] loss=2.98 avg=2.92\n",
            "[35 | 66.76] loss=3.72 avg=2.95\n",
            "[36 | 68.50] loss=2.48 avg=2.93\n",
            "[37 | 70.24] loss=2.47 avg=2.92\n",
            "[38 | 71.99] loss=2.81 avg=2.91\n",
            "[39 | 73.73] loss=5.06 avg=2.98\n",
            "[40 | 75.49] loss=2.30 avg=2.96\n",
            "[41 | 77.26] loss=3.09 avg=2.96\n",
            "[42 | 79.03] loss=2.60 avg=2.95\n",
            "[43 | 80.79] loss=2.06 avg=2.93\n",
            "[44 | 82.57] loss=2.96 avg=2.93\n",
            "[45 | 84.36] loss=1.89 avg=2.90\n",
            "[46 | 86.15] loss=2.94 avg=2.90\n",
            "[47 | 87.93] loss=2.18 avg=2.88\n",
            "[48 | 89.73] loss=2.94 avg=2.88\n",
            "[49 | 91.51] loss=2.88 avg=2.88\n",
            "[50 | 93.29] loss=2.44 avg=2.87\n",
            "[51 | 95.05] loss=2.75 avg=2.87\n",
            "[52 | 96.81] loss=1.92 avg=2.84\n",
            "[53 | 98.57] loss=2.92 avg=2.85\n",
            "[54 | 100.33] loss=3.27 avg=2.86\n",
            "[55 | 102.06] loss=1.99 avg=2.84\n",
            "[56 | 103.80] loss=2.79 avg=2.84\n",
            "[57 | 105.52] loss=2.89 avg=2.84\n",
            "[58 | 107.23] loss=2.32 avg=2.82\n",
            "[59 | 108.95] loss=2.10 avg=2.81\n",
            "[60 | 110.65] loss=2.47 avg=2.80\n",
            "[61 | 112.37] loss=2.70 avg=2.80\n",
            "[62 | 114.07] loss=2.77 avg=2.80\n",
            "[63 | 115.77] loss=3.53 avg=2.81\n",
            "[64 | 117.47] loss=2.66 avg=2.81\n",
            "[65 | 119.17] loss=3.13 avg=2.82\n",
            "[66 | 120.86] loss=1.92 avg=2.80\n",
            "[67 | 122.55] loss=2.62 avg=2.80\n",
            "[68 | 124.26] loss=3.42 avg=2.81\n",
            "[69 | 125.95] loss=2.86 avg=2.81\n",
            "[70 | 127.64] loss=2.55 avg=2.80\n",
            "[71 | 129.32] loss=3.44 avg=2.82\n",
            "[72 | 131.01] loss=2.42 avg=2.81\n",
            "[73 | 132.70] loss=2.31 avg=2.80\n",
            "[74 | 134.39] loss=3.24 avg=2.81\n",
            "[75 | 136.08] loss=2.90 avg=2.81\n",
            "[76 | 137.78] loss=2.70 avg=2.81\n",
            "[77 | 139.47] loss=2.03 avg=2.79\n",
            "[78 | 141.16] loss=2.42 avg=2.79\n",
            "[79 | 142.87] loss=2.70 avg=2.78\n",
            "[80 | 144.58] loss=2.40 avg=2.78\n",
            "[81 | 146.28] loss=3.12 avg=2.78\n",
            "[82 | 147.99] loss=3.37 avg=2.79\n",
            "[83 | 149.71] loss=2.15 avg=2.78\n",
            "[84 | 151.41] loss=2.75 avg=2.78\n",
            "[85 | 153.12] loss=2.32 avg=2.77\n",
            "[86 | 154.84] loss=3.65 avg=2.79\n",
            "[87 | 156.56] loss=3.72 avg=2.80\n",
            "[88 | 158.29] loss=3.28 avg=2.81\n",
            "[89 | 160.01] loss=2.49 avg=2.81\n",
            "[90 | 161.74] loss=2.32 avg=2.80\n",
            "[91 | 163.48] loss=3.37 avg=2.81\n",
            "[92 | 165.20] loss=3.47 avg=2.82\n",
            "[93 | 166.92] loss=2.65 avg=2.82\n",
            "[94 | 168.65] loss=2.48 avg=2.81\n",
            "[95 | 170.40] loss=3.85 avg=2.83\n",
            "[96 | 172.13] loss=3.51 avg=2.84\n",
            "[97 | 173.86] loss=2.44 avg=2.83\n",
            "[98 | 175.59] loss=2.61 avg=2.83\n",
            "[99 | 177.31] loss=2.44 avg=2.82\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " co-founder, CEO and CTO of BitBuck\n",
            "Rami Ismail\n",
            "Brenton Gant\n",
            "Rashid Hashim\n",
            "Ravi Kumar\n",
            "Vicky Bhatia\n",
            "Shailesh Kaur\n",
            "Nishdeep Kumar\n",
            "Zoe Devens\n",
            "Ajay Sareen\n",
            "Rajesh Kumar\n",
            "Pavan Sharma\n",
            "Mohit Singh\n",
            "Ravi Kumar\n",
            "Mohammad B.\n",
            "Yasin Khan\n",
            "Shailesh\n",
            "Ekaterina Makriva\n",
            "Manish Pandey\n",
            "Saurabh Vora\n",
            "Eldemarie\n",
            "Shailesh\n",
            "Bharat Raj\n",
            "Sanjay Singh\n",
            "Dhishal Choudhin\n",
            "Harsh Gauramangi\n",
            "Rafael Sol\n",
            "Shivajit Mukherji\n",
            "Bengali\n",
            "Jaitley\n",
            "Sanjay Singh\n",
            "Prajna Maitreya\n",
            "Gautam Ghosh\n",
            "Gautam Gupta\n",
            "Ravi Kumar\n",
            "Pankaj\n",
            "Nayantaran\n",
            "Samantha Lohal\n",
            "Amal Agarwal\n",
            "Zakia Masi\n",
            "Rupesh Koli\n",
            "Uday Kumar Singh\n",
            "Sikhsinh\n",
            "Adriana Kala\n",
            "Yashwant Sharma\n",
            "Samantha Lohal\n",
            "Nathalie\n",
            "Abhinav\n",
            "Amil\n",
            "Adhikari\n",
            "Ishwar Kalyan\n",
            "Mukesh Ambani\n",
            "Anis Chaudhari\n",
            "Prabhat Singh\n",
            "Kartika\n",
            "Laleshwar Singh\n",
            "Siddar Singh\n",
            "Bijay Khel\n",
            "Dhondra\n",
            "Yamish Goyal\n",
            "Bijayan Kumar Gupta\n",
            "Shashank Joshi\n",
            "Anand Sharma\n",
            "Manish Pandey\n",
            "Vani Durgodia\n",
            "Dipakash Rao\n",
            "Rajeev\n",
            "Vijayan\n",
            "Anand Sharma\n",
            "Srithaji\n",
            "Umar Bajpai\n",
            "Rachna Gupta\n",
            "Ashwini\n",
            "Gyanvendra Singh\n",
            "Gyanvendra Singh\n",
            "Rajesh\n",
            "Bjarni Gopal\n",
            "Bijay Khel\n",
            "Srinivasa Thi\n",
            "Prashant Singh\n",
            "Aditya Gupta\n",
            "Bajai Bajpai\n",
            "Rajesh Kumar\n",
            "Shivat\n",
            "Rajesh Kumar\n",
            "Gaurav\n",
            "Dhishan Gupta\n",
            "Anwar Shah\n",
            "Shailesh Kumar\n",
            "Babu Singh\n",
            "Raijit Singh\n",
            "Shailesh Kumar\n",
            "Danielsa Singh\n",
            "Mukesh Ambani\n",
            "Chennai\n",
            "Prashant Singh\n",
            "Jitender Sharma\n",
            "Shri Ram\n",
            "Dinesh\n",
            "Rupesh Koli\n",
            "Nadeem\n",
            "Deshadri Gupta\n",
            "Mamata Banerji\n",
            "Shruti Sharma\n",
            "Uday Kumar Singh\n",
            "Dhishan Gupta\n",
            "Krishna Sharma\n",
            "Shivat\n",
            "Dhondra\n",
            "Vijayan\n",
            "Sanjay Singh\n",
            "Jai Ashok\n",
            "Arvind Kejriwal\n",
            "Shivat\n",
            "Babu Singh\n",
            "Fazlur Rahman\n",
            "Sarup Raija\n",
            "Dhanyal Singh\n",
            "Gopinath Singh\n",
            "Lalwinder Singh\n",
            "Chidambaram\n",
            "Bajai Bajpai\n",
            "Srithaji\n",
            "Jaitley\n",
            "Shailesh Kumar\n",
            "Lalash Singh\n",
            "Baijit\n",
            "Shiva Singh\n",
            "Pankaj\n",
            "Aamir Khan\n",
            "Shailesh Kumar\n",
            "Sushma Sharma\n",
            "Rajesh Kumar\n",
            "Vijayan\n",
            "Anand\n",
            "Siddar Singh\n",
            "Bipin Rai\n",
            "Sanjay Singh\n",
            "Karthik\n",
            "Jai Ashok\n",
            "Shivaprasad\n",
            "Nand\n",
            "Nandini\n",
            "Bhagat Singh\n",
            "Ramesh Sharma\n",
            "Rajesh Kumar\n",
            "Dhanshaw\n",
            "Sajjan Gavle\n",
            "Chitra Prakash\n",
            "Rishi Sharma\n",
            "Vijayan\n",
            "Aishwarya Singh\n",
            "Shivaprasad\n",
            "Faisaluddin Shah\n",
            "Bhagat Singh\n",
            "Vijayan\n",
            "Shivaprasad\n",
            "Faisal Siddiqui\n",
            "Sushma Sharma\n",
            "Harsh V\n",
            "Rishi Sharma\n",
            "Shivaprasad\n",
            "Vijayan\n",
            "Anand\n",
            "Siddar Singh\n",
            "Shivaprasad\n",
            "Vijayan\n",
            "Dhanshaw\n",
            "Faisaluddin Shah\n",
            "Abhay\n",
            "Vijayan\n",
            "Dhinu\n",
            "Vijayan\n",
            "Shivaprasad\n",
            "Shivanam Singh\n",
            "Sushma Sharma\n",
            "Vijayan\n",
            "Sushma Sharma\n",
            "Shivaprasad\n",
            "Vijayan\n",
            "Rishi Sharma\n",
            "Vijayan\n",
            "Vijayan\n",
            "Vijayan\n",
            "Vijayan\n",
            "Rishi Sharma\n",
            "Sanjay Kumar\n",
            "Dhanshaw\n",
            "Vijayan\n",
            "Shivaprasad\n",
            "Sanjay Kumar\n",
            "Vijayan\n",
            "Vijayan\n",
            "Vijayan\n",
            "Vijayan\n",
            "Hindu\n",
            "Prakash\n",
            "Krishna\n",
            "Tiffany\n",
            "Krishna\n",
            "Shivanam\n",
            "Ek\n",
            "\n",
            "[100 | 203.84] loss=2.17 avg=2.81\n",
            "[101 | 205.58] loss=2.75 avg=2.81\n",
            "[102 | 207.30] loss=3.33 avg=2.82\n",
            "[103 | 209.02] loss=3.65 avg=2.83\n",
            "[104 | 210.75] loss=2.83 avg=2.83\n",
            "[105 | 212.47] loss=2.97 avg=2.84\n",
            "[106 | 214.19] loss=3.67 avg=2.85\n",
            "[107 | 215.91] loss=2.85 avg=2.85\n",
            "[108 | 217.64] loss=2.23 avg=2.84\n",
            "[109 | 219.36] loss=3.46 avg=2.85\n",
            "[110 | 221.08] loss=2.60 avg=2.84\n",
            "[111 | 222.80] loss=2.95 avg=2.85\n",
            "[112 | 224.53] loss=3.26 avg=2.85\n",
            "[113 | 226.25] loss=2.42 avg=2.85\n",
            "[114 | 227.97] loss=3.22 avg=2.85\n",
            "[115 | 229.69] loss=2.47 avg=2.85\n",
            "[116 | 231.42] loss=3.06 avg=2.85\n",
            "[117 | 233.15] loss=3.40 avg=2.86\n",
            "[118 | 234.90] loss=2.30 avg=2.85\n",
            "[119 | 236.64] loss=3.27 avg=2.85\n",
            "[120 | 238.38] loss=3.02 avg=2.86\n",
            "[121 | 240.12] loss=3.44 avg=2.87\n",
            "[122 | 241.87] loss=3.64 avg=2.88\n",
            "[123 | 243.61] loss=3.64 avg=2.89\n",
            "[124 | 245.36] loss=3.57 avg=2.90\n",
            "[125 | 247.10] loss=4.07 avg=2.91\n",
            "[126 | 248.84] loss=2.32 avg=2.90\n",
            "[127 | 250.60] loss=3.43 avg=2.91\n",
            "[128 | 252.34] loss=3.54 avg=2.92\n",
            "[129 | 254.09] loss=1.97 avg=2.91\n",
            "[130 | 255.83] loss=2.82 avg=2.91\n",
            "[131 | 257.56] loss=2.21 avg=2.90\n",
            "[132 | 259.31] loss=3.73 avg=2.91\n",
            "[133 | 261.05] loss=3.18 avg=2.91\n",
            "[134 | 262.79] loss=2.91 avg=2.91\n",
            "[135 | 264.53] loss=3.35 avg=2.92\n",
            "[136 | 266.26] loss=2.45 avg=2.91\n",
            "[137 | 268.01] loss=2.78 avg=2.91\n",
            "[138 | 269.74] loss=2.65 avg=2.91\n",
            "[139 | 271.48] loss=3.12 avg=2.91\n",
            "[140 | 273.22] loss=3.06 avg=2.91\n",
            "[141 | 274.97] loss=2.28 avg=2.90\n",
            "[142 | 276.71] loss=2.39 avg=2.90\n",
            "[143 | 278.45] loss=2.10 avg=2.89\n",
            "[144 | 280.17] loss=2.67 avg=2.88\n",
            "[145 | 281.89] loss=3.03 avg=2.88\n",
            "[146 | 283.62] loss=3.81 avg=2.90\n",
            "[147 | 285.35] loss=3.11 avg=2.90\n",
            "[148 | 287.07] loss=2.87 avg=2.90\n",
            "[149 | 288.81] loss=2.52 avg=2.89\n",
            "[150 | 290.55] loss=2.29 avg=2.89\n",
            "[151 | 292.28] loss=2.88 avg=2.89\n",
            "[152 | 294.01] loss=2.62 avg=2.88\n",
            "[153 | 295.75] loss=2.59 avg=2.88\n",
            "[154 | 297.49] loss=2.87 avg=2.88\n",
            "[155 | 299.22] loss=2.95 avg=2.88\n",
            "[156 | 300.94] loss=2.20 avg=2.87\n",
            "[157 | 302.66] loss=2.71 avg=2.87\n",
            "[158 | 304.38] loss=2.43 avg=2.86\n",
            "[159 | 306.11] loss=2.85 avg=2.86\n",
            "[160 | 307.83] loss=3.04 avg=2.87\n",
            "[161 | 309.55] loss=3.02 avg=2.87\n",
            "[162 | 311.28] loss=2.85 avg=2.87\n",
            "[163 | 312.99] loss=2.86 avg=2.87\n",
            "[164 | 314.72] loss=3.39 avg=2.87\n",
            "[165 | 316.43] loss=3.08 avg=2.88\n",
            "[166 | 318.16] loss=2.45 avg=2.87\n",
            "[167 | 319.88] loss=2.61 avg=2.87\n",
            "[168 | 321.60] loss=4.22 avg=2.88\n",
            "[169 | 323.34] loss=2.01 avg=2.87\n",
            "[170 | 325.07] loss=2.66 avg=2.87\n",
            "[171 | 326.79] loss=2.43 avg=2.87\n",
            "[172 | 328.51] loss=3.01 avg=2.87\n",
            "[173 | 330.24] loss=2.65 avg=2.86\n",
            "[174 | 331.96] loss=2.63 avg=2.86\n",
            "[175 | 333.68] loss=3.14 avg=2.87\n",
            "[176 | 335.40] loss=2.64 avg=2.86\n",
            "[177 | 337.12] loss=3.83 avg=2.87\n",
            "[178 | 338.84] loss=3.48 avg=2.88\n",
            "[179 | 340.58] loss=2.36 avg=2.88\n",
            "[180 | 342.32] loss=2.45 avg=2.87\n",
            "[181 | 344.06] loss=2.97 avg=2.87\n",
            "[182 | 345.78] loss=3.04 avg=2.87\n",
            "[183 | 347.49] loss=3.40 avg=2.88\n",
            "[184 | 349.22] loss=1.85 avg=2.87\n",
            "[185 | 350.95] loss=3.02 avg=2.87\n",
            "[186 | 352.69] loss=2.20 avg=2.86\n",
            "[187 | 354.42] loss=2.95 avg=2.86\n",
            "[188 | 356.15] loss=3.04 avg=2.86\n",
            "[189 | 357.89] loss=2.84 avg=2.86\n",
            "[190 | 359.61] loss=2.72 avg=2.86\n",
            "[191 | 361.34] loss=2.94 avg=2.86\n",
            "[192 | 363.08] loss=3.27 avg=2.87\n",
            "[193 | 364.81] loss=2.74 avg=2.87\n",
            "[194 | 366.56] loss=2.41 avg=2.86\n",
            "[195 | 368.29] loss=2.92 avg=2.86\n",
            "[196 | 370.01] loss=2.67 avg=2.86\n",
            "[197 | 371.74] loss=3.83 avg=2.87\n",
            "[198 | 373.47] loss=3.04 avg=2.87\n",
            "[199 | 375.19] loss=2.31 avg=2.87\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " ima biz\n",
            "\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "\n",
            "this song is so hot\n",
            "\n",
            "imma show me the place\n",
            "\n",
            "imma show me the place\n",
            "\n",
            "\n",
            "it is only when I am here\n",
            "\n",
            "imma show me the place\n",
            "\n",
            "\n",
            "this song is so hot\n",
            "\n",
            "imma show me the place\n",
            "\n",
            "imma show me the place\n",
            "\n",
            "\n",
            "this song is so hot\n",
            "\n",
            "imma show me the place\n",
            "\n",
            "imma show me the place\n",
            "\n",
            "\n",
            "this song is so hot\n",
            "\n",
            "imma show me the place\n",
            "\n",
            "imma show me the place\n",
            "\n",
            "\n",
            "this song is so hot\n",
            "\n",
            "imma show me the place\n",
            "\n",
            "imma show me the place\n",
            "\n",
            "\n",
            "this song is so great\n",
            "\n",
            "this song is so dope\n",
            "\n",
            "this song is so hot\n",
            "\n",
            "imma show me the place\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "it is only when i am here\n",
            "\n",
            "imma show to me the place\n",
            "\n",
            "imma show you the place\n",
            "\n",
            "\n",
            "this song is so hot\n",
            "\n",
            "imma show me the place\n",
            "\n",
            "imma show you the place\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "imma show\n",
            "\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz imariz\n",
            "\n",
            "imma biz imábiz\n",
            "\n",
            "imma biz imádiz\n",
            "\n",
            "imma biz imbíz\n",
            "\n",
            "imma aintir\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma abi biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "imma biz ima biz\n",
            "\n",
            "\n",
            "[200 | 398.94] loss=2.54 avg=2.86\n",
            "[201 | 400.67] loss=2.81 avg=2.86\n",
            "[202 | 402.41] loss=2.37 avg=2.86\n",
            "[203 | 404.13] loss=2.26 avg=2.85\n",
            "[204 | 405.85] loss=2.43 avg=2.85\n",
            "[205 | 407.56] loss=2.98 avg=2.85\n",
            "[206 | 409.29] loss=2.99 avg=2.85\n",
            "[207 | 410.99] loss=2.58 avg=2.85\n",
            "[208 | 412.71] loss=3.10 avg=2.85\n",
            "[209 | 414.43] loss=2.18 avg=2.84\n",
            "[210 | 416.15] loss=2.03 avg=2.83\n",
            "[211 | 417.86] loss=2.68 avg=2.83\n",
            "[212 | 419.58] loss=2.46 avg=2.83\n",
            "[213 | 421.28] loss=2.50 avg=2.82\n",
            "[214 | 423.01] loss=2.69 avg=2.82\n",
            "[215 | 424.73] loss=3.05 avg=2.82\n",
            "[216 | 426.45] loss=3.01 avg=2.82\n",
            "[217 | 428.17] loss=2.63 avg=2.82\n",
            "[218 | 429.89] loss=3.39 avg=2.83\n",
            "[219 | 431.62] loss=2.73 avg=2.83\n",
            "[220 | 433.36] loss=2.48 avg=2.82\n",
            "[221 | 435.09] loss=2.58 avg=2.82\n",
            "[222 | 436.81] loss=3.30 avg=2.83\n",
            "[223 | 438.55] loss=1.93 avg=2.82\n",
            "[224 | 440.28] loss=2.66 avg=2.81\n",
            "[225 | 442.03] loss=2.92 avg=2.82\n",
            "[226 | 443.77] loss=3.08 avg=2.82\n",
            "[227 | 445.50] loss=2.85 avg=2.82\n",
            "[228 | 447.24] loss=1.98 avg=2.81\n",
            "[229 | 448.98] loss=2.39 avg=2.81\n",
            "[230 | 450.72] loss=2.42 avg=2.80\n",
            "[231 | 452.46] loss=2.78 avg=2.80\n",
            "[232 | 454.20] loss=3.38 avg=2.81\n",
            "[233 | 455.95] loss=2.84 avg=2.81\n",
            "[234 | 457.69] loss=2.70 avg=2.81\n",
            "[235 | 459.43] loss=2.59 avg=2.80\n",
            "[236 | 461.17] loss=4.20 avg=2.82\n",
            "[237 | 462.91] loss=2.07 avg=2.81\n",
            "[238 | 464.65] loss=2.15 avg=2.80\n",
            "[239 | 466.39] loss=2.04 avg=2.80\n",
            "[240 | 468.14] loss=2.08 avg=2.79\n",
            "[241 | 469.88] loss=3.18 avg=2.79\n",
            "[242 | 471.64] loss=2.89 avg=2.79\n",
            "[243 | 473.40] loss=2.91 avg=2.79\n",
            "[244 | 475.14] loss=2.22 avg=2.79\n",
            "[245 | 476.88] loss=2.82 avg=2.79\n",
            "[246 | 478.62] loss=2.72 avg=2.79\n",
            "[247 | 480.36] loss=3.35 avg=2.79\n",
            "[248 | 482.11] loss=2.61 avg=2.79\n",
            "[249 | 483.85] loss=2.87 avg=2.79\n",
            "[250 | 485.59] loss=3.10 avg=2.80\n",
            "[251 | 487.34] loss=2.35 avg=2.79\n",
            "[252 | 489.08] loss=2.59 avg=2.79\n",
            "[253 | 490.83] loss=2.99 avg=2.79\n",
            "[254 | 492.59] loss=3.51 avg=2.80\n",
            "[255 | 494.33] loss=2.48 avg=2.80\n",
            "[256 | 496.07] loss=2.09 avg=2.79\n",
            "[257 | 497.81] loss=2.58 avg=2.79\n",
            "[258 | 499.55] loss=2.89 avg=2.79\n",
            "[259 | 501.29] loss=2.45 avg=2.78\n",
            "[260 | 503.03] loss=3.32 avg=2.79\n",
            "[261 | 504.77] loss=3.70 avg=2.80\n",
            "[262 | 506.52] loss=3.13 avg=2.80\n",
            "[263 | 508.26] loss=3.11 avg=2.81\n",
            "[264 | 510.00] loss=3.13 avg=2.81\n",
            "[265 | 511.74] loss=2.77 avg=2.81\n",
            "[266 | 513.49] loss=2.63 avg=2.81\n",
            "[267 | 515.22] loss=2.58 avg=2.80\n",
            "[268 | 516.96] loss=4.15 avg=2.82\n",
            "[269 | 518.71] loss=2.76 avg=2.82\n",
            "[270 | 520.47] loss=2.47 avg=2.81\n",
            "[271 | 522.21] loss=3.25 avg=2.82\n",
            "[272 | 523.95] loss=2.92 avg=2.82\n",
            "[273 | 525.69] loss=2.33 avg=2.82\n",
            "[274 | 527.44] loss=1.85 avg=2.80\n",
            "[275 | 529.18] loss=2.40 avg=2.80\n",
            "[276 | 530.92] loss=3.46 avg=2.81\n",
            "[277 | 532.66] loss=2.13 avg=2.80\n",
            "[278 | 534.41] loss=2.86 avg=2.80\n",
            "[279 | 536.16] loss=2.73 avg=2.80\n",
            "[280 | 537.91] loss=3.69 avg=2.81\n",
            "[281 | 539.65] loss=2.41 avg=2.81\n",
            "[282 | 541.39] loss=3.12 avg=2.81\n",
            "[283 | 543.13] loss=2.47 avg=2.81\n",
            "[284 | 544.87] loss=3.27 avg=2.81\n",
            "[285 | 546.62] loss=2.42 avg=2.81\n",
            "[286 | 548.36] loss=2.10 avg=2.80\n",
            "[287 | 550.12] loss=3.58 avg=2.81\n",
            "[288 | 551.86] loss=2.59 avg=2.80\n",
            "[289 | 553.61] loss=3.71 avg=2.81\n",
            "[290 | 555.35] loss=2.80 avg=2.81\n",
            "[291 | 557.10] loss=3.32 avg=2.82\n",
            "[292 | 558.84] loss=2.62 avg=2.82\n",
            "[293 | 560.58] loss=2.79 avg=2.82\n",
            "[294 | 562.32] loss=3.28 avg=2.82\n",
            "[295 | 564.06] loss=2.31 avg=2.82\n",
            "[296 | 565.80] loss=4.10 avg=2.83\n",
            "[297 | 567.54] loss=2.91 avg=2.83\n",
            "[298 | 569.28] loss=2.29 avg=2.82\n",
            "[299 | 571.02] loss=3.79 avg=2.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "researched for your needs\n",
            "and i can teach you the way to live\n",
            "in your little tiny town\n",
            "i teach you to eat all the damn\n",
            "i teach you to eat all the damn\n",
            "i teach you to eat all the damn\n",
            "i teach you to be me\n",
            "i teach you i teach you I teach you\n",
            "i teach you it tells me everything\n",
            "i know where you need me i know where you need me\n",
            "i can help you find love\n",
            "i want to help you find love\n",
            "i want to help you find love\n",
            "i need you but I don't think ill have a clue\n",
            "i dont want you now youre so beautiful\n",
            "but i think youre so beautiful\n",
            "i believe that youre so beautiful youve got the kind of charm\n",
            "i think youre so beautiful youve got the kind of charm\n",
            "i believe that youre so beautiful youve got the kind of charm\n",
            "i believe that youre so beautiful youve got the kind of charm\n",
            "i believe that youre so beautiful youve got the kind of charm\n",
            "i cant see you without seeing me\n",
            "im gonna see you tomorrow so i know thats okay\n",
            "i tell the truth to my soul and I have a reason\n",
            "and for you no one is gonna know so i\n",
            "can't see you without seeing me\n",
            "i cant see you without seeing me\n",
            "i cant see you without seeing me\n",
            "<|endoftext|>\n",
            "we met over the kitchen stove\n",
            "some little old lady\n",
            "and he took up my old shoes\n",
            "i took up his old hat\n",
            "she never knew me\n",
            "the shoes were gone i had to do it\n",
            "but we found time to go on in a day\n",
            "dinner went cold when we passed each other\n",
            "dinner went cold when we passed each other when i passed each other\n",
            "dinner went cold when we passed each other\n",
            "dinner went cold when we passed each other\n",
            "dinner went cold when we passed each other\n",
            "<|endoftext|>\n",
            "what about that guy in the church\n",
            "what about that little town\n",
            "he has a job and a wife\n",
            "and he wears a suit and a tie\n",
            "he has a wife and two little girls\n",
            "he is a good man he is not a bad man\n",
            "it would be rude of me to talk of these things\n",
            "but some people say he is kind\n",
            "but some people say he is really bad\n",
            "for if they say he is kind\n",
            "they must be lying\n",
            "and his words are so bad\n",
            "he has been an imbecile\n",
            "his words so wrong\n",
            "for if they say he is kind\n",
            "they must be lying\n",
            "but some people say he is kind\n",
            "he has been an imbecile he has been an imbecile\n",
            "he has been an imbecile\n",
            "it would be the height of a fool for him to be\n",
            "for he is a fool but his soul looks like a man\n",
            "the words he says are so evil\n",
            "no one wants to be in the company\n",
            "of someone like that\n",
            "<|endoftext|>\n",
            "you will always leave with my arms wrapped around you\n",
            "your arms holding on to me so tight\n",
            "but you know that i am yours\n",
            "you have left a mark on this earth\n",
            "all you will remember is me\n",
            "you will remember the little things\n",
            "the way you kissed me that i never knew\n",
            "your breath on my lips that made me remember you\n",
            "oh baby baby baby\n",
            "you said the right words to the right girl\n",
            "and i said the same thing to the right girl\n",
            "now let the sun shine on us\n",
            "let the sun shine the little things\n",
            "oh baby baby baby\n",
            "you said the right words to the right girl\n",
            "and i said the same thing to the right girl\n",
            "now let the sun shine on us\n",
            "let the sun shine the little things\n",
            "oh baby baby baby\n",
            "<|endoftext|>\n",
            "i had the time of my life\n",
            "i had the time to do anything\n",
            "so i let you out and went down the street\n",
            "the sun was shining\n",
            "and i wanted to be at you\n",
            "but all i could see was you\n",
            "my arms held out to you as if im a kid\n",
            "my lips were so shy\n",
            "your arms around me as soon as i turned around\n",
            "i didn't want to tell you\n",
            "i knew that you would remember me\n",
            "i kept my fingers open to see what the hell your hands were doing\n",
            "trying to get your hair to match yours\n",
            "and im a little girl\n",
            "not an adult\n",
            "my arms held out to you in a way that you didnt hear\n",
            "my lips were so shy\n",
            "my eyes shining like little girls\n",
            "dying to tell you this\n",
            "i heard you say your name and all i could think of was you\n",
            "theres nothing in the world of love\n",
            "it was all i knew was you\n",
            "your arms around me and it was all i knew was you\n",
            "<|endoftext|>\n",
            "my friends call it a dream come true\n",
            "it feels so heavenly\n",
            "just like being\n",
            "\n",
            "[300 | 594.92] loss=3.44 avg=2.84\n",
            "[301 | 596.65] loss=2.46 avg=2.84\n",
            "[302 | 598.37] loss=2.36 avg=2.83\n",
            "[303 | 600.10] loss=3.30 avg=2.84\n",
            "[304 | 601.82] loss=2.84 avg=2.84\n",
            "[305 | 603.53] loss=4.36 avg=2.85\n",
            "[306 | 605.24] loss=2.50 avg=2.85\n",
            "[307 | 606.96] loss=2.41 avg=2.85\n",
            "[308 | 608.67] loss=2.71 avg=2.84\n",
            "[309 | 610.40] loss=3.24 avg=2.85\n",
            "[310 | 612.12] loss=2.98 avg=2.85\n",
            "[311 | 613.83] loss=2.07 avg=2.84\n",
            "[312 | 615.55] loss=2.94 avg=2.84\n",
            "[313 | 617.25] loss=1.93 avg=2.83\n",
            "[314 | 618.95] loss=3.42 avg=2.84\n",
            "[315 | 620.66] loss=2.63 avg=2.84\n",
            "[316 | 622.38] loss=2.30 avg=2.83\n",
            "[317 | 624.11] loss=2.22 avg=2.82\n",
            "[318 | 625.83] loss=2.66 avg=2.82\n",
            "[319 | 627.56] loss=3.26 avg=2.83\n",
            "[320 | 629.28] loss=3.18 avg=2.83\n",
            "[321 | 631.01] loss=2.26 avg=2.82\n",
            "[322 | 632.75] loss=2.91 avg=2.83\n",
            "[323 | 634.49] loss=2.42 avg=2.82\n",
            "[324 | 636.24] loss=2.20 avg=2.82\n",
            "[325 | 637.98] loss=2.57 avg=2.81\n",
            "[326 | 639.73] loss=2.81 avg=2.81\n",
            "[327 | 641.47] loss=2.48 avg=2.81\n",
            "[328 | 643.21] loss=2.92 avg=2.81\n",
            "[329 | 644.95] loss=3.19 avg=2.81\n",
            "[330 | 646.70] loss=3.46 avg=2.82\n",
            "[331 | 648.46] loss=2.94 avg=2.82\n",
            "[332 | 650.20] loss=3.40 avg=2.83\n",
            "[333 | 651.94] loss=3.84 avg=2.84\n",
            "[334 | 653.68] loss=2.89 avg=2.84\n",
            "[335 | 655.42] loss=3.05 avg=2.84\n",
            "[336 | 657.16] loss=3.34 avg=2.85\n",
            "[337 | 658.91] loss=2.30 avg=2.84\n",
            "[338 | 660.65] loss=2.84 avg=2.84\n",
            "[339 | 662.39] loss=2.95 avg=2.84\n",
            "[340 | 664.13] loss=3.54 avg=2.85\n",
            "[341 | 665.87] loss=2.71 avg=2.85\n",
            "[342 | 667.61] loss=3.41 avg=2.85\n",
            "[343 | 669.35] loss=2.77 avg=2.85\n",
            "[344 | 671.09] loss=2.34 avg=2.85\n",
            "[345 | 672.84] loss=2.31 avg=2.84\n",
            "[346 | 674.57] loss=2.43 avg=2.84\n",
            "[347 | 676.31] loss=3.48 avg=2.84\n",
            "[348 | 678.03] loss=2.91 avg=2.84\n",
            "[349 | 679.75] loss=4.32 avg=2.86\n",
            "[350 | 681.48] loss=2.24 avg=2.85\n",
            "[351 | 683.21] loss=1.86 avg=2.84\n",
            "[352 | 684.95] loss=2.78 avg=2.84\n",
            "[353 | 686.67] loss=2.78 avg=2.84\n",
            "[354 | 688.40] loss=2.87 avg=2.84\n",
            "[355 | 690.12] loss=3.73 avg=2.85\n",
            "[356 | 691.85] loss=3.80 avg=2.86\n",
            "[357 | 693.59] loss=2.36 avg=2.86\n",
            "[358 | 695.32] loss=2.52 avg=2.85\n",
            "[359 | 697.03] loss=2.40 avg=2.85\n",
            "[360 | 698.76] loss=2.81 avg=2.85\n",
            "[361 | 700.49] loss=2.71 avg=2.85\n",
            "[362 | 702.23] loss=2.73 avg=2.84\n",
            "[363 | 703.96] loss=2.75 avg=2.84\n",
            "[364 | 705.68] loss=1.90 avg=2.83\n",
            "[365 | 707.42] loss=2.84 avg=2.83\n",
            "[366 | 709.14] loss=3.28 avg=2.84\n",
            "[367 | 710.87] loss=2.18 avg=2.83\n",
            "[368 | 712.59] loss=1.97 avg=2.82\n",
            "[369 | 714.31] loss=3.25 avg=2.83\n",
            "[370 | 716.05] loss=2.07 avg=2.82\n",
            "[371 | 717.79] loss=2.53 avg=2.82\n",
            "[372 | 719.52] loss=3.22 avg=2.82\n",
            "[373 | 721.24] loss=2.33 avg=2.82\n",
            "[374 | 722.96] loss=3.15 avg=2.82\n",
            "[375 | 724.68] loss=2.77 avg=2.82\n",
            "[376 | 726.40] loss=1.74 avg=2.81\n",
            "[377 | 728.14] loss=2.82 avg=2.81\n",
            "[378 | 729.86] loss=2.85 avg=2.81\n",
            "[379 | 731.61] loss=2.28 avg=2.80\n",
            "[380 | 733.35] loss=2.24 avg=2.80\n",
            "[381 | 735.09] loss=2.64 avg=2.80\n",
            "[382 | 736.83] loss=2.39 avg=2.79\n",
            "[383 | 738.55] loss=3.20 avg=2.80\n",
            "[384 | 740.30] loss=4.06 avg=2.81\n",
            "[385 | 742.04] loss=2.19 avg=2.80\n",
            "[386 | 743.78] loss=2.20 avg=2.80\n",
            "[387 | 745.52] loss=2.60 avg=2.79\n",
            "[388 | 747.25] loss=3.07 avg=2.80\n",
            "[389 | 748.99] loss=2.66 avg=2.80\n",
            "[390 | 750.73] loss=3.21 avg=2.80\n",
            "[391 | 752.47] loss=2.85 avg=2.80\n",
            "[392 | 754.21] loss=3.44 avg=2.81\n",
            "[393 | 755.96] loss=2.49 avg=2.80\n",
            "[394 | 757.70] loss=2.57 avg=2.80\n",
            "[395 | 759.43] loss=2.95 avg=2.80\n",
            "[396 | 761.15] loss=2.16 avg=2.80\n",
            "[397 | 762.89] loss=2.35 avg=2.79\n",
            "[398 | 764.64] loss=3.41 avg=2.80\n",
            "[399 | 766.38] loss=3.13 avg=2.80\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " named\n",
            "\n",
            "that you should sing and dance and be with me tonight\n",
            "and it hurts my heart to hear\n",
            "<|endoftext|>\n",
            "and I want my love with me tonight\n",
            "i want my love with me tonight\n",
            "<|endoftext|>\n",
            "youre going to the gym\n",
            "youve got your shit together\n",
            "youre going to get it\n",
            "cause i dont think thats fair\n",
            "your eyes dont look real nice\n",
            "your arms dont look real nice\n",
            "and your little blue eyes dont look so bad\n",
            "i didnt do those pictures\n",
            "i dont remember the last time\n",
            "oh maybe you dont remember\n",
            "maybe youre afraid\n",
            "your eyes dont look real nice\n",
            "your arms dont look real nice\n",
            "oh maybe you dont recall\n",
            "maybe you dont know\n",
            "the last time i knew this\n",
            "oh maybe you dont remember\n",
            "my mind is a mirror\n",
            "but the eyes dont look even that nice\n",
            "maybe you dont remember\n",
            "i didnt do those pictures\n",
            "i dont remember the last time\n",
            "oh maybe you dont remember\n",
            "yeah maybe you dont remember\n",
            "but your head is a mirror\n",
            "and the eyes dont look even that nice\n",
            "maybe you dont recall\n",
            "that i was always like that\n",
            "<|endoftext|>\n",
            "we dont need to tell\n",
            "we should keep it private\n",
            "and that aint our style\n",
            "we need to keep our emotions private\n",
            "all alone with our emotions inside\n",
            "like the two most beautiful girls i have ever known\n",
            "we need to keep it private\n",
            "and i dont want too much more\n",
            "and i dont want to see\n",
            "youre the center of our hearts\n",
            "we need to keep our emotions private\n",
            "and that aint our style\n",
            "we need to keep our emotions private\n",
            "all alone with our emotions inside\n",
            "like the two most beautiful girls i have ever known\n",
            "we need to keep it private\n",
            "and i dont want too much more\n",
            "and i dont want to see\n",
            "youre our center of our hearts\n",
            "we need to keep our emotions private\n",
            "and that aint our style\n",
            "we need to keep our emotions private\n",
            "all alone with our emotions inside\n",
            "like the two most beautiful girls\n",
            "we need to keep it private\n",
            "and i dont want too much more\n",
            "and i dont want to see\n",
            "youre our center of our hearts\n",
            "we need to keep our emotions private\n",
            "and we dont want too much more\n",
            "and i dont want to see\n",
            "<|endoftext|>\n",
            "my friends say that youve been here and done that\n",
            "but they dont show us that youre out there\n",
            "i got to start from the beginning\n",
            "i want to start from now\n",
            "i am still waiting\n",
            "for a friend to make me feel okay\n",
            "but i cant seem to find a friend\n",
            "i keep on asking where i was\n",
            "and i keep on asking why\n",
            "i want to know what is in your heart\n",
            "and i keep on asking because\n",
            "you are the reason i live\n",
            "i want to live my life well\n",
            "and this is the truth\n",
            "i want to understand\n",
            "that there are no friends\n",
            "i keep on wanting to know the truth\n",
            "you were what i needed\n",
            "but you didnt take me seriously\n",
            "and i kept on calling\n",
            "but i didnt believe you\n",
            "for that one night i went to bed\n",
            "but when you came along\n",
            "i didnt want to go\n",
            "i was never really like\n",
            "this kind of love\n",
            "i needed\n",
            "but my heart is still waiting\n",
            "for a friend to make me feel okay\n",
            "but i cant seem to find one\n",
            "i keep on asking why\n",
            "where you are\n",
            "if you want to go\n",
            "because youre the person that i need\n",
            "but you dont look anywhere\n",
            "i keep on asking\n",
            "why is it that you dont talk to me\n",
            "you keep on calling\n",
            "but i dont believe you\n",
            "we could be\n",
            "this love\n",
            "this love\n",
            "this love\n",
            "i can feel you\n",
            "just you know\n",
            "oh i cant seem to hear you\n",
            "my friends\n",
            "i want to start from the beginning\n",
            "i want to start from now\n",
            "i am still waiting\n",
            "for a friend to make me feel okay\n",
            "but i cant seem to find one\n",
            "i keep on asking\n",
            "if you want to go\n",
            "cause youre the person that i need\n",
            "but you dont look anywhere\n",
            "i keep on asking\n",
            "why is it that you dont talk to me\n",
            "you keep on calling\n",
            "but i dont believe you\n",
            "for that one night i gone to bed\n",
            "but when you came along\n",
            "i said i needed you and i need you\n",
            "but you dont look anywhere\n",
            "i keep on asking\n",
            "why is it that you dont talk to me\n",
            "you keep on calling\n",
            "but i dont believe you\n",
            "for that one night i went to bed\n",
            "but when you came along\n",
            "we had a feeling\n",
            "but it didnt lead anywhere\n",
            "i keep on asking\n",
            "why is it that you dont talk to me\n",
            "i keep on calling\n",
            "but i dont believe you\n",
            "for that one night i went to bed\n",
            "but when you came along\n",
            "i said i needed you and i need\n",
            "\n",
            "[400 | 790.20] loss=3.21 avg=2.81\n",
            "[401 | 791.93] loss=2.55 avg=2.80\n",
            "[402 | 793.65] loss=3.71 avg=2.81\n",
            "[403 | 795.37] loss=2.42 avg=2.81\n",
            "[404 | 797.09] loss=2.58 avg=2.81\n",
            "[405 | 798.81] loss=2.80 avg=2.81\n",
            "[406 | 800.53] loss=2.83 avg=2.81\n",
            "[407 | 802.25] loss=2.35 avg=2.80\n",
            "[408 | 803.98] loss=2.77 avg=2.80\n",
            "[409 | 805.70] loss=2.61 avg=2.80\n",
            "[410 | 807.40] loss=2.62 avg=2.80\n",
            "[411 | 809.12] loss=2.67 avg=2.80\n",
            "[412 | 810.84] loss=2.36 avg=2.79\n",
            "[413 | 812.56] loss=1.73 avg=2.78\n",
            "[414 | 814.28] loss=3.00 avg=2.78\n",
            "[415 | 816.01] loss=3.33 avg=2.79\n",
            "[416 | 817.73] loss=2.48 avg=2.79\n",
            "[417 | 819.45] loss=3.01 avg=2.79\n",
            "[418 | 821.18] loss=2.67 avg=2.79\n",
            "[419 | 822.91] loss=3.35 avg=2.79\n",
            "[420 | 824.65] loss=2.49 avg=2.79\n",
            "[421 | 826.40] loss=2.37 avg=2.79\n",
            "[422 | 828.14] loss=2.00 avg=2.78\n",
            "[423 | 829.89] loss=2.59 avg=2.78\n",
            "[424 | 831.62] loss=3.27 avg=2.78\n",
            "[425 | 833.37] loss=2.28 avg=2.78\n",
            "[426 | 835.12] loss=3.72 avg=2.78\n",
            "[427 | 836.87] loss=3.71 avg=2.79\n",
            "[428 | 838.63] loss=2.38 avg=2.79\n",
            "[429 | 840.38] loss=2.66 avg=2.79\n",
            "[430 | 842.11] loss=3.55 avg=2.80\n",
            "[431 | 843.86] loss=1.95 avg=2.79\n",
            "[432 | 845.60] loss=3.02 avg=2.79\n",
            "[433 | 847.34] loss=2.95 avg=2.79\n",
            "[434 | 849.09] loss=2.27 avg=2.79\n",
            "[435 | 850.83] loss=1.57 avg=2.77\n",
            "[436 | 852.57] loss=2.82 avg=2.77\n",
            "[437 | 854.31] loss=2.09 avg=2.77\n",
            "[438 | 856.05] loss=2.71 avg=2.77\n",
            "[439 | 857.80] loss=2.60 avg=2.77\n",
            "[440 | 859.54] loss=3.22 avg=2.77\n",
            "[441 | 861.28] loss=2.21 avg=2.76\n",
            "[442 | 863.02] loss=2.40 avg=2.76\n",
            "[443 | 864.76] loss=3.01 avg=2.76\n",
            "[444 | 866.48] loss=4.32 avg=2.78\n",
            "[445 | 868.21] loss=2.98 avg=2.78\n",
            "[446 | 869.95] loss=2.27 avg=2.78\n",
            "[447 | 871.67] loss=3.28 avg=2.78\n",
            "[448 | 873.41] loss=2.65 avg=2.78\n",
            "[449 | 875.15] loss=2.84 avg=2.78\n",
            "[450 | 876.89] loss=2.78 avg=2.78\n",
            "[451 | 878.63] loss=2.63 avg=2.78\n",
            "[452 | 880.37] loss=2.83 avg=2.78\n",
            "[453 | 882.10] loss=2.80 avg=2.78\n",
            "[454 | 883.84] loss=2.43 avg=2.78\n",
            "[455 | 885.59] loss=2.58 avg=2.77\n",
            "[456 | 887.31] loss=2.08 avg=2.77\n",
            "[457 | 889.04] loss=2.09 avg=2.76\n",
            "[458 | 890.76] loss=1.87 avg=2.75\n",
            "[459 | 892.48] loss=3.64 avg=2.76\n",
            "[460 | 894.20] loss=2.37 avg=2.76\n",
            "[461 | 895.92] loss=2.57 avg=2.75\n",
            "[462 | 897.65] loss=2.33 avg=2.75\n",
            "[463 | 899.39] loss=2.18 avg=2.74\n",
            "[464 | 901.11] loss=2.80 avg=2.74\n",
            "[465 | 902.83] loss=3.01 avg=2.75\n",
            "[466 | 904.56] loss=2.79 avg=2.75\n",
            "[467 | 906.28] loss=2.55 avg=2.75\n",
            "[468 | 908.00] loss=3.63 avg=2.75\n",
            "[469 | 909.74] loss=2.35 avg=2.75\n",
            "[470 | 911.48] loss=2.44 avg=2.75\n",
            "[471 | 913.22] loss=2.25 avg=2.74\n",
            "[472 | 914.94] loss=2.28 avg=2.74\n",
            "[473 | 916.67] loss=2.91 avg=2.74\n",
            "[474 | 918.39] loss=3.35 avg=2.75\n",
            "[475 | 920.11] loss=2.76 avg=2.75\n",
            "[476 | 921.85] loss=2.80 avg=2.75\n",
            "[477 | 923.58] loss=2.95 avg=2.75\n",
            "[478 | 925.30] loss=2.72 avg=2.75\n",
            "[479 | 927.01] loss=2.17 avg=2.74\n",
            "[480 | 928.74] loss=2.53 avg=2.74\n",
            "[481 | 930.46] loss=2.55 avg=2.74\n",
            "[482 | 932.18] loss=2.79 avg=2.74\n",
            "[483 | 933.90] loss=2.86 avg=2.74\n",
            "[484 | 935.63] loss=2.54 avg=2.74\n",
            "[485 | 937.34] loss=2.22 avg=2.73\n",
            "[486 | 939.06] loss=2.99 avg=2.74\n",
            "[487 | 940.79] loss=2.17 avg=2.73\n",
            "[488 | 942.53] loss=3.25 avg=2.74\n",
            "[489 | 944.26] loss=2.79 avg=2.74\n",
            "[490 | 945.98] loss=3.13 avg=2.74\n",
            "[491 | 947.70] loss=2.66 avg=2.74\n",
            "[492 | 949.42] loss=2.44 avg=2.74\n",
            "[493 | 951.14] loss=2.39 avg=2.73\n",
            "[494 | 952.85] loss=2.36 avg=2.73\n",
            "[495 | 954.59] loss=2.70 avg=2.73\n",
            "[496 | 956.31] loss=3.49 avg=2.74\n",
            "[497 | 958.03] loss=1.79 avg=2.73\n",
            "[498 | 959.75] loss=3.01 avg=2.73\n",
            "[499 | 961.47] loss=2.45 avg=2.73\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "the sun always shines on my face\n",
            "but the sky is black\n",
            "thats just who I am\n",
            "its just who you will find\n",
            "thats just who you will find\n",
            "thats just who you will find\n",
            "its just who you will find\n",
            "it always seems like were talking to the heavens\n",
            "<|endoftext|>\n",
            "when you see a man that is your kind \n",
            "and youre afraid\n",
            "and when you see a man that is your kind \n",
            "that wouldnt be a thing that isnt\n",
            "if you knew the truth\n",
            "you would not have to be afraid\n",
            "because of the things that youve done\n",
            "that youve done\n",
            "if you never know the truth \n",
            "then youve got nothing to be afraid of\n",
            "that youve done\n",
            "that youve done \n",
            "if you never know the truth \n",
            "if you had a heart \n",
            "that you wouldnt have to be afraid \n",
            "about \n",
            "the things that youve done \n",
            "that youve done\n",
            "youve done \n",
            "if you never know the truth\n",
            "<|endoftext|>\n",
            "soul can move me\n",
            "its just the two of us\n",
            "the wind can pick a fight now\n",
            "sitting beside the hearth fire\n",
            "the fire can hear us go\n",
            "the hearth fire is so heavy\n",
            "it needs help now\n",
            "sitting beside the hearth fire\n",
            "the fire can hear us go\n",
            "the hearth fire is so heavy\n",
            "it needs help now\n",
            "sitting beside the hearth fire\n",
            "the hearth fire\n",
            "can't you see it in your heart\n",
            "the flames burning with you around your mouth\n",
            "sitting beside the hearth fire\n",
            "the fire can hear us go\n",
            "sitting beside the hearth fire\n",
            "the hearth fire\n",
            "can't you see it in your heart\n",
            "and the flames burning with you around your mouth\n",
            "sitting beside the hearth fire\n",
            "the hearth fire\n",
            "can't you see it in your heart\n",
            "the flames burning with you around your mouth\n",
            "sitting beside the hearth fire\n",
            "the hearth fire\n",
            "just the two of us\n",
            "<|endoftext|>\n",
            "so the night is cold\n",
            "now ive just seen a light\n",
            "so i feel like i belong to your house\n",
            "im just a child dreaming of your love \n",
            "so the night is lonely\n",
            "its a lonely night in thistles\n",
            "i wouldnt call you my mother\n",
            "<|endoftext|>\n",
            "welcome to the world\n",
            "of the past\n",
            "of the past\n",
            "we hold on to our memories\n",
            "its not all roses and joys\n",
            "but now i know what youve taken away\n",
            "the sky is blue\n",
            "the sky is blue\n",
            "the sky is blue\n",
            "welcome to the world\n",
            "of the past\n",
            "of the past\n",
            "welcome to the world \n",
            "welcome to the world that is now\n",
            "<|endoftext|>\n",
            "when i go on out of love\n",
            "i know that my heart does belong to you\n",
            "so take care\n",
            "oh baby you got me soo fast\n",
            "well i dont mean that thats why i cry\n",
            "its the only way i know how\n",
            "i would never dream that you wouldnt be home with me right now\n",
            "i wouldnt even know you wouldnt be\n",
            "it would hurt a little to walk away\n",
            "but its love that makes me believe there is something more to you\n",
            "it makes me believe there is nothing more to me\n",
            "that makes me believe there is a world\n",
            "i dont mean that thats why i cry\n",
            "its the only way i know how\n",
            "i would never dream that you wouldnt be home with me right now\n",
            "i wouldnt even know you wouldnt be\n",
            "welcome to the world\n",
            "of the past\n",
            "of the past\n",
            "welcome to the world \n",
            "welcome to the world that is now\n",
            "welcome to the world of the past\n",
            "<|endoftext|>\n",
            "hey little baby baby\n",
            "i dont know when i first kissed you\n",
            "but i know you feel like it all\n",
            "hey little baby baby\n",
            "i dont know when we first met\n",
            "but that day when i couldnt stop kissing you\n",
            "i couldnt stop looking at you\n",
            "hey little baby baby\n",
            "i wonder why you love me\n",
            "ill be your best friend forever and ever\n",
            "hey little baby baby\n",
            "weve been inlove for so long\n",
            "i dont know when we last started\n",
            "im not sure why i dont know what it it means\n",
            "what it means\n",
            "dont wanna hear it ill be on your mind\n",
            "hey little baby baby\n",
            "and i dont know when i first kissed you\n",
            "but i know you feel like it all\n",
            "hey little baby baby\n",
            "i dont know when we last started\n",
            "im not sure what it it means\n",
            "what it means\n",
            "dont wanna hear it ill be on your mind\n",
            "hey little baby baby\n",
            "and i dont know when we last started\n",
            "well it doesnt really matter baby im not gonna say no\n",
            "its one thing for me\n",
            "i dont find you\n",
            "\n",
            "[500 | 985.30] loss=2.03 avg=2.72\n",
            "[501 | 987.03] loss=3.22 avg=2.72\n",
            "[502 | 988.73] loss=2.59 avg=2.72\n",
            "[503 | 990.46] loss=3.06 avg=2.73\n",
            "[504 | 992.18] loss=2.02 avg=2.72\n",
            "[505 | 993.90] loss=3.16 avg=2.72\n",
            "[506 | 995.62] loss=2.22 avg=2.72\n",
            "[507 | 997.34] loss=2.97 avg=2.72\n",
            "[508 | 999.06] loss=3.22 avg=2.73\n",
            "[509 | 1000.78] loss=2.77 avg=2.73\n",
            "[510 | 1002.51] loss=2.80 avg=2.73\n",
            "[511 | 1004.23] loss=2.57 avg=2.73\n",
            "[512 | 1005.95] loss=3.48 avg=2.73\n",
            "[513 | 1007.67] loss=3.56 avg=2.74\n",
            "[514 | 1009.39] loss=3.24 avg=2.75\n",
            "[515 | 1011.11] loss=2.02 avg=2.74\n",
            "[516 | 1012.83] loss=2.86 avg=2.74\n",
            "[517 | 1014.55] loss=2.57 avg=2.74\n",
            "[518 | 1016.28] loss=2.51 avg=2.74\n",
            "[519 | 1018.01] loss=2.57 avg=2.74\n",
            "[520 | 1019.72] loss=2.64 avg=2.73\n",
            "[521 | 1021.45] loss=3.09 avg=2.74\n",
            "[522 | 1023.18] loss=3.42 avg=2.74\n",
            "[523 | 1024.90] loss=2.85 avg=2.75\n",
            "[524 | 1026.62] loss=2.33 avg=2.74\n",
            "[525 | 1028.34] loss=2.87 avg=2.74\n",
            "[526 | 1030.07] loss=2.15 avg=2.74\n",
            "[527 | 1031.81] loss=3.32 avg=2.74\n",
            "[528 | 1033.55] loss=2.36 avg=2.74\n",
            "[529 | 1035.28] loss=2.21 avg=2.73\n",
            "[530 | 1037.00] loss=2.45 avg=2.73\n",
            "[531 | 1038.74] loss=2.60 avg=2.73\n",
            "[532 | 1040.47] loss=2.57 avg=2.73\n",
            "[533 | 1042.22] loss=2.59 avg=2.73\n",
            "[534 | 1043.96] loss=2.55 avg=2.72\n",
            "[535 | 1045.71] loss=2.30 avg=2.72\n",
            "[536 | 1047.45] loss=3.14 avg=2.72\n",
            "[537 | 1049.18] loss=2.24 avg=2.72\n",
            "[538 | 1050.93] loss=2.83 avg=2.72\n",
            "[539 | 1052.67] loss=2.01 avg=2.71\n",
            "[540 | 1054.41] loss=1.90 avg=2.71\n",
            "[541 | 1056.15] loss=2.55 avg=2.70\n",
            "[542 | 1057.90] loss=2.59 avg=2.70\n",
            "[543 | 1059.64] loss=2.47 avg=2.70\n",
            "[544 | 1061.38] loss=2.62 avg=2.70\n",
            "[545 | 1063.12] loss=3.22 avg=2.70\n",
            "[546 | 1064.86] loss=2.61 avg=2.70\n",
            "[547 | 1066.61] loss=2.53 avg=2.70\n",
            "[548 | 1068.35] loss=2.38 avg=2.70\n",
            "[549 | 1070.09] loss=2.69 avg=2.70\n",
            "[550 | 1071.85] loss=2.13 avg=2.69\n",
            "[551 | 1073.59] loss=2.59 avg=2.69\n",
            "[552 | 1075.34] loss=2.56 avg=2.69\n",
            "[553 | 1077.08] loss=2.34 avg=2.69\n",
            "[554 | 1078.82] loss=2.79 avg=2.69\n",
            "[555 | 1080.58] loss=3.47 avg=2.70\n",
            "[556 | 1082.32] loss=3.12 avg=2.70\n",
            "[557 | 1084.05] loss=2.11 avg=2.69\n",
            "[558 | 1085.80] loss=2.86 avg=2.70\n",
            "[559 | 1087.55] loss=3.80 avg=2.71\n",
            "[560 | 1089.29] loss=2.73 avg=2.71\n",
            "[561 | 1091.03] loss=2.48 avg=2.70\n",
            "[562 | 1092.77] loss=2.89 avg=2.71\n",
            "[563 | 1094.52] loss=2.40 avg=2.70\n",
            "[564 | 1096.25] loss=2.42 avg=2.70\n",
            "[565 | 1098.00] loss=2.98 avg=2.70\n",
            "[566 | 1099.74] loss=2.05 avg=2.70\n",
            "[567 | 1101.48] loss=2.64 avg=2.70\n",
            "[568 | 1103.23] loss=2.63 avg=2.70\n",
            "[569 | 1104.99] loss=2.24 avg=2.69\n",
            "[570 | 1106.73] loss=4.18 avg=2.71\n",
            "[571 | 1108.47] loss=2.22 avg=2.70\n",
            "[572 | 1110.22] loss=2.57 avg=2.70\n",
            "[573 | 1111.96] loss=2.14 avg=2.69\n",
            "[574 | 1113.70] loss=2.14 avg=2.69\n",
            "[575 | 1115.44] loss=2.29 avg=2.68\n",
            "[576 | 1117.20] loss=3.81 avg=2.70\n",
            "[577 | 1118.94] loss=3.13 avg=2.70\n",
            "[578 | 1120.68] loss=2.64 avg=2.70\n",
            "[579 | 1122.42] loss=2.45 avg=2.70\n",
            "[580 | 1124.16] loss=3.89 avg=2.71\n",
            "[581 | 1125.90] loss=2.21 avg=2.70\n",
            "[582 | 1127.65] loss=2.57 avg=2.70\n",
            "[583 | 1129.39] loss=2.86 avg=2.70\n",
            "[584 | 1131.13] loss=2.23 avg=2.70\n",
            "[585 | 1132.87] loss=2.03 avg=2.69\n",
            "[586 | 1134.62] loss=2.48 avg=2.69\n",
            "[587 | 1136.34] loss=2.72 avg=2.69\n",
            "[588 | 1138.08] loss=2.52 avg=2.69\n",
            "[589 | 1139.82] loss=2.95 avg=2.69\n",
            "[590 | 1141.56] loss=3.18 avg=2.70\n",
            "[591 | 1143.31] loss=3.07 avg=2.70\n",
            "[592 | 1145.05] loss=3.04 avg=2.70\n",
            "[593 | 1146.79] loss=2.40 avg=2.70\n",
            "[594 | 1148.52] loss=3.01 avg=2.70\n",
            "[595 | 1150.25] loss=3.11 avg=2.71\n",
            "[596 | 1151.99] loss=2.46 avg=2.71\n",
            "[597 | 1153.73] loss=2.17 avg=2.70\n",
            "[598 | 1155.48] loss=3.13 avg=2.70\n",
            "[599 | 1157.22] loss=2.39 avg=2.70\n",
            "Generating samples...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS1RJJDFOPnb",
        "colab_type": "text"
      },
      "source": [
        "## Save Checkpoints\n",
        "Save our checkpoints to start training again later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oZ9GeFAw5k6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/ /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-i7vERWbNS",
        "colab_type": "text"
      },
      "source": [
        "## 3. Testing\n",
        "\n",
        "Load your trained model for use in sampling below (117M or 345M)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeETvWvrbKga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/117M/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np0r6qfXBeUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/345M/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmnSrXqtfRbq",
        "colab_type": "text"
      },
      "source": [
        "Generate conditional samples from the model given a prompt you provide -  change top-k hyperparameter if desired (default is 40),  if you're using 345M, add \"--model-name 345M\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utJj-iY4gHwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --top_k 40 --model_name \"345M\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeDhY97XMDXn",
        "colab_type": "text"
      },
      "source": [
        "To check flag descriptions, use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBaj2L_KMAgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py -- --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8rSqkGxg5OK",
        "colab_type": "text"
      },
      "source": [
        "Generate unconditional samples from the model,  if you're using 345M, add \"--model-name 345M\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaQUEnRxWc3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/generate_unconditional_samples.py --model_name \"345M\" --nsamples 3 | tee /tmp/samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM1Hag-JL3Bt",
        "colab_type": "text"
      },
      "source": [
        "To check flag descriptions, use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdxfye-SL66I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/generate_unconditional_samples.py -- --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psTTPq_z61uK",
        "colab_type": "text"
      },
      "source": [
        "Check output similarity with the dataset.  [(source)](https://jmonlong.github.io/Hippocamplus/2018/04/16/text-similarity/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4akfvmy56h4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/jmonlong/Hippocamplus/config/python/simText/simText.py\n",
        "!python2 simText.py -s 0.5 -1 dataset.txt -2 /tmp/samples "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}